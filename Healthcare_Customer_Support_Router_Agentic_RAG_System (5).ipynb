{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZbflrcEGi8U"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.3.20\n",
        "!pip install langchain-openai==0.3.9\n",
        "!pip install langchain-community==0.3.20\n",
        "!pip install langgraph==0.3.18"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-chroma==0.2.2"
      ],
      "metadata": {
        "id": "9Cq1bW42G8i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipywidgets\n",
        "!pip install jupyter-ui-poll==1.0.0"
      ],
      "metadata": {
        "id": "nlz87SSnHJtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ],
      "metadata": {
        "id": "aKQfKGC3HNL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.environ[\"OPENAI_API_KEY\"][:10])"
      ],
      "metadata": {
        "id": "XglC0xQGJXwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import os\n",
        "import logging\n",
        "\n",
        "os.environ[\"CHROMA_TELEMETRY\"] = \"FALSE\"\n",
        "\n",
        "#logging.getLogger(\"chromadb.telemetry\").setLevel(logging.CRITICAL)\n",
        "#logging.getLogger(\"chromadb.telemetry.product.posthog\").setLevel(logging.CRITICAL)\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n"
      ],
      "metadata": {
        "id": "OOZNVrK0HURv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or download manually from https://drive.google.com/file/d/1_bQj7VkXDMwwqJmspFgRzH2mgK1CVMUY/view?usp=sharing and upload to colab or your notebook location\n",
        "!gdown 1_bQj7VkXDMwwqJmspFgRzH2mgK1CVMUY"
      ],
      "metadata": {
        "id": "NvxJ_7kOHdRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"./healthcare_db.json\", \"r\") as f:\n",
        "    knowledge_base = json.load(f)\n",
        "\n",
        "knowledge_base[:4]"
      ],
      "metadata": {
        "id": "du7M5_DjHitL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_base[-3:]"
      ],
      "metadata": {
        "id": "InlE_gBIHw01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Store as LangChain Documents**"
      ],
      "metadata": {
        "id": "po5F4eLrIS16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "from tqdm import tqdm\n",
        "\n",
        "processed_docs = []\n",
        "\n",
        "for doc in tqdm(knowledge_base):\n",
        "    metadata = doc['metadata']\n",
        "    data = doc['text']\n",
        "    processed_docs.append(Document(page_content=data,\n",
        "                                   metadata=metadata))\n",
        "\n",
        "processed_docs[:3]"
      ],
      "metadata": {
        "id": "TdMoU6CSH9lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "processed_docs[:-3]"
      ],
      "metadata": {
        "id": "4vakIuoDIB3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Vector DB**"
      ],
      "metadata": {
        "id": "cYjLNXJXIiWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ],
      "metadata": {
        "id": "BqfAGzhYIlhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, shutil\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "PERSIST_DIR = \"/content/knowledge_base_V3\"\n",
        "\n",
        "if os.path.exists(PERSIST_DIR):\n",
        "    shutil.rmtree(PERSIST_DIR)\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "kbase_db = Chroma.from_documents(\n",
        "    documents=processed_docs,\n",
        "    collection_name=\"knowledge_base_v3\",\n",
        "    embedding=openai_embed_model,\n",
        "    collection_metadata={\"hnsw:space\": \"cosine\"}, #indexing method hierarchial navigable small world. Graph based algorithm used in Approximate Nearest Neighbour (ANN)\n",
        "    persist_directory=\"./knowledge_base\")\n",
        "\n",
        "#from langchain_chroma import Chroma\n",
        "#Chroma.delete_collection()\n",
        "#kbase_db = Chroma.from_documents(documents=processed_docs,\n",
        " #                               collection_name='knowledge_base',\n",
        "  #                              embedding=openai_embed_model,\n",
        "   #                             collection_metadata={\"hnsw:space\": \"cosine\"}, #indexing method hierarchial navigable small world. Graph based algorithm used in Approximate Nearest Neighbour (ANN)\n",
        "    #                            persist_directory=\"./knowledge_base\")\n"
      ],
      "metadata": {
        "id": "c09jGP-UIkQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Vector Database Retriever Strategy**"
      ],
      "metadata": {
        "id": "TtUl3KJ1eouz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kbase_search = kbase_db.as_retriever(search_type=\"similarity_score_threshold\", #cosine similarity\n",
        "                                     search_kwargs={\"k\": 3, \"score_threshold\": 0.3}) # 3 - top 3 documents 0.3 = if any document is less than 30% cosine similarity, the document will be dropped"
      ],
      "metadata": {
        "id": "nE5xU0yVexkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TESTING THE RETRIEVER**"
      ],
      "metadata": {
        "id": "pcmASPSdiMvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query= 'How to book an appointment?'\n",
        "metadata_filter = {'category' : 'appointments'}\n",
        "kbase_search.search_kwargs[\"filter\"] = metadata_filter\n",
        "kbase_search.invoke(query)"
      ],
      "metadata": {
        "id": "7JBC34jjiMJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = kbase_db._collection.get(              # No filtering, fetches all documents\n",
        "    include=[\"metadatas\"]    # Include metadata in the result\n",
        ")"
      ],
      "metadata": {
        "id": "lByU-C2qtnwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the unique 'category' values from the metadata\n",
        "unique_categories = set()\n",
        "for metadata in results[\"metadatas\"]:\n",
        "    if \"category\" in metadata:\n",
        "        unique_categories.add(metadata[\"category\"])\n",
        "\n",
        "print(unique_categories)"
      ],
      "metadata": {
        "id": "I2wwSExttrd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BUILD ROUTER AGENTIC RAG **"
      ],
      "metadata": {
        "id": "tTFxX5ecOr4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 1 - Building the Agent Schema"
      ],
      "metadata": {
        "id": "4KoiPp-9O_Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Literal\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CustomerSupportAgentState(TypedDict):\n",
        "    customer_query: str\n",
        "    query_category: str\n",
        "    query_sentiment: str\n",
        "    escalation_cust_info: dict\n",
        "    oncall_cust_info: dict\n",
        "    final_response: str\n",
        "\n",
        "class QueryCategory(BaseModel):\n",
        "    categorized_topic: Literal['Billing', 'Appointments', 'Records', 'Insurance']\n",
        "\n",
        "class QuerySentiment(BaseModel):\n",
        "    sentiment: Literal['Positive', 'Neutral', 'Negative', 'Distress']"
      ],
      "metadata": {
        "id": "D2FxIOzOOreM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ],
      "metadata": {
        "id": "yp0a7RsrQWRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_inquiry(support_state: CustomerSupportAgentState) -> CustomerSupportAgentState:\n",
        "    \"\"\"\n",
        "    Classify the customer query into 'Billing', 'Appointments', 'Records' or 'Insurance'.\n",
        "    \"\"\"\n",
        "\n",
        "    query = support_state[\"customer_query\"]\n",
        "    ROUTE_CATEGORY_PROMPT = \"\"\"Act as a customer support agent trying to best categorize the customer query.\n",
        "                               You are a support agent for a healthcare company focusing on providing healthcare services to customers.\n",
        "                               These services include:\n",
        "                                - handling billing queries\n",
        "                                - handling insurance queries\n",
        "                                - handling appointment booking queries\n",
        "                                - handling medical records queries\n",
        "\n",
        "                               Please read the customer query below and\n",
        "                               determine the best category from the following list:\n",
        "\n",
        "                               'Billing', 'Appointments', 'Records' or 'Insurance'\n",
        "\n",
        "                               Remember:\n",
        "                                - Billing queries will focus more on payment and billing related aspects\n",
        "                                - Appointments will focus more on booking, rescheduling, cancelling doctor appointments\n",
        "                                - Records will focus more on updating, sharing, accessing medical records\n",
        "                                - Insurance will focus more on insurance related queries like claims, updating insurance policy details\n",
        "\n",
        "                                Return just the category name (from one of the above)\n",
        "\n",
        "                                Query:\n",
        "                                {customer_query}\n",
        "                            \"\"\"\n",
        "    prompt = ROUTE_CATEGORY_PROMPT.format(customer_query=query)\n",
        "    route_category = llm.with_structured_output(QueryCategory).invoke(prompt)\n",
        "\n",
        "\n",
        "    # Extract category from nested structure\n",
        "    if isinstance(route_category, dict) and 'values' in route_category:\n",
        "        # It's {'values': ['Category']} - extract first item from list\n",
        "        category = route_category['values'][0]\n",
        "    elif hasattr(route_category, 'categorized_topic'):\n",
        "        # Pydantic object with categorized_topic attribute\n",
        "        category = route_category.categorized_topic\n",
        "    elif hasattr(route_category, 'category'):\n",
        "        # Pydantic object with category attribute\n",
        "        category = route_category.category\n",
        "    elif isinstance(route_category, dict) and 'category' in route_category:\n",
        "        # Dict with 'category' key\n",
        "        category = route_category['category']\n",
        "    else:\n",
        "        # Unknown structure - use first available value\n",
        "        if isinstance(route_category, dict):\n",
        "            category = list(route_category.values())[0]\n",
        "            if isinstance(category, list):\n",
        "                category = category[0]\n",
        "        else:\n",
        "            category = \"Unknown\"\n",
        "\n",
        "    return {\n",
        "        \"query_category\": category\n",
        "    }"
      ],
      "metadata": {
        "id": "zMaIIuWaSD5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorize_inquiry({\"customer_query\": \"What doctors are available?\"})"
      ],
      "metadata": {
        "id": "5iDl6kAmY_A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorize_inquiry({\"customer_query\": \"What insurance is available?\"})"
      ],
      "metadata": {
        "id": "Iy02JazEZShB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_inquiry_sentiment(support_state: CustomerSupportAgentState) -> CustomerSupportAgentState:\n",
        "    \"\"\"\n",
        "    Classify the customer sentiment into 'Positive', 'Neutral', 'Negative', 'Distress'.\n",
        "    \"\"\"\n",
        "\n",
        "    query = support_state[\"customer_query\"]\n",
        "    SENTIMENT_CATEGORY_PROMPT = \"\"\"Act as a customer support agent trying to best categorize the sentiment of the customer query.\n",
        "                               You are a support agent for a healthcare company focusing on providing healthcare services to customers.\n",
        "                               These services include:\n",
        "                                - handling billing queries\n",
        "                                - handling insurance queries\n",
        "                                - handling appointment booking queries\n",
        "                                - handling medical records queries\n",
        "\n",
        "                               Please read the customer query below and\n",
        "                               determine the best category from the following list:\n",
        "\n",
        "                               'Positive', 'Neutral', 'Negative', 'Distress'\n",
        "\n",
        "\n",
        "                                Remember these rules when finding the sentiment:\n",
        "                                - 'Distress' happens only when the customer is facing a health emergency and might need the on-call emergency doctor\n",
        "                                - 'Negative' happens only when the customer is not happy with certain products, services offered by the company\n",
        "\n",
        "\n",
        "                                Return just the category name (from one of the above)\n",
        "\n",
        "                                Query:\n",
        "                                {customer_query}\n",
        "                            \"\"\"\n",
        "    prompt = SENTIMENT_CATEGORY_PROMPT.format(customer_query=query)\n",
        "    sentiment_result = llm.with_structured_output(QuerySentiment).invoke(prompt)\n",
        "\n",
        "    # Extract sentiment from nested structure\n",
        "    if isinstance(sentiment_result, dict) and 'values' in sentiment_result:\n",
        "        sentiment = sentiment_result['values'][0]\n",
        "    elif hasattr(sentiment_result, 'sentiment'):\n",
        "        sentiment = sentiment_result.sentiment\n",
        "    elif isinstance(sentiment_result, dict) and 'sentiment' in sentiment_result:\n",
        "        sentiment = sentiment_result['sentiment']\n",
        "    else:\n",
        "        if isinstance(sentiment_result, dict):\n",
        "            sentiment = list(sentiment_result.values())[0]\n",
        "            if isinstance(sentiment, list):\n",
        "                sentiment = sentiment[0]\n",
        "        else:\n",
        "            sentiment = \"Neutral\"\n",
        "\n",
        "    return {\n",
        "        \"query_sentiment\": sentiment\n",
        "    }"
      ],
      "metadata": {
        "id": "DRTCyoa7ZY_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_inquiry_sentiment({\"customer_query\": \"I'm unable to breathe, need help\"})"
      ],
      "metadata": {
        "id": "FYlgx6AIaNIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from typing import Dict\n",
        "\n",
        "def generate_department_response(support_state: CustomerSupportAgentState) -> CustomerSupportAgentState:\n",
        "    \"\"\"\n",
        "    Provide a department support response by combining knowledge from the vector store and LLM.\n",
        "    \"\"\"\n",
        "    # Retrieve category and ensure it is lowercase for metadata filtering\n",
        "    categorized_topic = support_state[\"query_category\"]\n",
        "    query = support_state[\"customer_query\"]\n",
        "\n",
        "    # Use metadata filter for department - specific queries\n",
        "    if categorized_topic.lower() == \"billing\":\n",
        "        metadata_filter = {\"category\": \"billing\"}\n",
        "        department = \"Billing\"\n",
        "    elif categorized_topic.lower() == \"appointments\":\n",
        "        metadata_filter = {\"category\": \"appointments\"}\n",
        "        department = \"Appointments\"\n",
        "    elif categorized_topic.lower() == \"records\":\n",
        "        metadata_filter = {\"category\": \"medical_records\"}\n",
        "        department = \"Medical Records\"\n",
        "    elif categorized_topic.lower() == \"insurance\":\n",
        "        metadata_filter = {\"category\": \"insurance\"}\n",
        "        department = \"Insurance\"\n",
        "    # apply metadata filter\n",
        "    kbase_search.search_kwargs[\"filter\"] = metadata_filter\n",
        "\n",
        "    # Perform retrieval from VectorDB\n",
        "    relevant_docs = kbase_search.invoke(query)\n",
        "    retrieved_content = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "\n",
        "    # Combine retrieved information into the prompt\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        Craft a clear and detailed support response for the following customer query about {department}.\n",
        "        Use the provided knowledge base information to enrich your response.\n",
        "        In case there is no knowledge base information or you do not know the answer just say:\n",
        "\n",
        "        Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\n",
        "\n",
        "        Customer Query:\n",
        "        {customer_query}\n",
        "\n",
        "        Relevant Knowledge Base Information:\n",
        "        {retrieved_content}\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Generate the final response using the LLM\n",
        "    chain = prompt | llm\n",
        "    reply = chain.invoke({\n",
        "        \"customer_query\": query,\n",
        "        \"retrieved_content\": retrieved_content,\n",
        "        \"department\": department\n",
        "    }).content\n",
        "\n",
        "    # Update and return the modified support state\n",
        "    return {\n",
        "        \"final_response\": reply\n",
        "    }"
      ],
      "metadata": {
        "id": "rLVh6UgEe5Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "754f5df7"
      },
      "source": [
        "display(processed_docs[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_department_response({\"customer_query\": \"what payment modes do you accept?\", \"query_category\": \"Billing\"})"
      ],
      "metadata": {
        "id": "iEmD1OIle8tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from jupyter_ui_poll import ui_events\n",
        "import time\n",
        "\n",
        "def accept_user_input_escalation(support_state: CustomerSupportAgentState) -> CustomerSupportAgentState:\n",
        "\n",
        "# ‚úÖ CHECK FOR TEST MODE - Skip form if testing\n",
        "    if support_state.get(\"test_mode\", False):\n",
        "        return {\n",
        "            'escalation_cust_info': {\n",
        "                'name': 'TEST_USER',\n",
        "                'number': '000-000-0000',\n",
        "                'email': 'test@example.com'\n",
        "            }\n",
        "        }\n",
        "    # REMEMBER: You can always customize the way you accept user input by modifying the code below\n",
        "    # here we use jupyter widgets so you don't have to install too many external dependencies\n",
        "\n",
        "    global form_submitted  # status variable to track form submission\n",
        "    form_submitted = False # initially form hasn't been submitted\n",
        "\n",
        "    # UI Header: A simple HTML element to label the form\n",
        "    header = widgets.HTML(\"<h3>Escalation Form - Please enter your details below:</h3>\")\n",
        "\n",
        "    # Text input fields to collect user information\n",
        "    input1 = widgets.Text(description='Name:')     # User's full name\n",
        "    input2 = widgets.Text(description='Number:')   # Contact number\n",
        "    input3 = widgets.Text(description='Email:')    # Email address\n",
        "\n",
        "    # Dictionary to store form responses after submission\n",
        "    result = {}\n",
        "\n",
        "    # Callback function to be triggered when the Submit button is clicked\n",
        "    def on_submit(submit_button):\n",
        "        global form_submitted\n",
        "        form_submitted = True  # Mark the form as submitted\n",
        "        # Store user inputs into the result dictionary\n",
        "        result['name'] = input1.value\n",
        "        result['number'] = input2.value\n",
        "        result['email'] = input3.value\n",
        "        # Provide visual feedback that form is submitted\n",
        "        submit_button.description = 'üëç'\n",
        "\n",
        "    # Submit button widget setup\n",
        "    submit_button = widgets.Button(description=\"Submit\")\n",
        "    submit_button.on_click(on_submit)  # Attach callback to button\n",
        "\n",
        "    # Combine all widgets into a vertical layout box\n",
        "    vbox = widgets.VBox([header, input1, input2, input3, submit_button])\n",
        "    display(vbox)  # Render the form in the notebook interface\n",
        "\n",
        "    # Keep polling UI events until the form is submitted\n",
        "    with ui_events() as poll:\n",
        "        while form_submitted is False:\n",
        "            poll(5)               # Listen for UI events\n",
        "            print('.', end='')   # Show a dot to indicate waiting for input from user\n",
        "            time.sleep(0.3)      # Slight delay to reduce CPU usage\n",
        "\n",
        "    # Return updated agent state with captured user info for escalation\n",
        "    return {\n",
        "        'escalation_cust_info': result\n",
        "    }\n"
      ],
      "metadata": {
        "id": "6NOKO1A4hFRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accept_user_input_escalation({})"
      ],
      "metadata": {
        "id": "chDJ7zk_iE72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def escalate_to_human_agent(support_state: CustomerSupportAgentState) -> CustomerSupportAgentState:\n",
        "\n",
        "    # REMEMBER: You can always customize the way you notify the human support agent by adding custom code below.\n",
        "    # This could include emailing them, paging them, sending them notifications using specific platform APIs like whatsapp etc.\n",
        "    # Here we have kept it very simple:\n",
        "    #  we just show a response back to the user showing the details they entered in the form earlier\n",
        "    #  and telling them they will be contacted by a human support agent\n",
        "\n",
        "    # get the customer info from agent state which they entered in the form\n",
        "    escalation_cust_info = support_state['escalation_cust_info']\n",
        "    # the following response will be shown to the user and can also be sent (customer form inputs) to your human support agents\n",
        "    response = (\"Apologies, \" + escalation_cust_info['name'] +\n",
        "                \",  we are really sorry! Someone from our team will be reaching out to via email shortly at \"+\n",
        "                escalation_cust_info['email'] + \" and if needed we will also be calling you at: \" +\n",
        "                escalation_cust_info['number'] + \" to help you out!\")\n",
        "\n",
        "    # NOTE: You can always add custom code here to call specific APIs like whatsapp or email etc to notify your human support agents\n",
        "\n",
        "    return {\n",
        "        \"final_response\": response\n",
        "    }"
      ],
      "metadata": {
        "id": "aeBpEp9N5vef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accept_user_input_oncall(support_state: CustomerSupportAgentState) -> CustomerSupportAgentState:\n",
        " # ‚úÖ CHECK FOR TEST MODE - Skip form if testing\n",
        "  if support_state.get(\"test_mode\", False):\n",
        "        return {\n",
        "            'escalation_cust_info': {\n",
        "                'name': 'TEST_USER',\n",
        "                'number': '000-000-0000',\n",
        "                'email': 'test@example.com'\n",
        "            }\n",
        "        }\n",
        "\n",
        "  global form_submitted  # status variable to track form submission\n",
        "  form_submitted = False # initially form hasn't been submitted\n",
        "\n",
        "  #Header\n",
        "  header = widgets.HTML(\"<h3>Emergency Form - Please enter your details below:</h3>\")\n",
        "\n",
        "  # Text input fields to collect critical user info\n",
        "  input1 = widgets.Text(description='Name:')     # User's full name\n",
        "  input2 = widgets.Text(description='Number:')   # Contact number (required for emergency callback)\n",
        "\n",
        "  # Dictionary to store the captured input values\n",
        "  result = {}\n",
        "\n",
        "  def on_submit(submit_button):\n",
        "      global form_submitted\n",
        "      form_submitted = True  # Form is now submitted\n",
        "      # Save form data to result dictionar\n",
        "      result['name'] = input1.value\n",
        "      result['number'] = input2.value\n",
        "      # Visual confirmation of submission\n",
        "      submit_button.description = 'üëç'\n",
        "\n",
        "  # Submit button widget setup\n",
        "  submit_button = widgets.Button(description=\"Submit\")\n",
        "  submit_button.on_click(on_submit)  # Attach the callback function\n",
        "\n",
        "  # Combine widgets vertically and render in notebook\n",
        "  vbox = widgets.VBox([header, input1, input2, submit_button])\n",
        "  display(vbox)\n",
        "\n",
        "  #Keep polling for UI events until form is submitted\n",
        "  with ui_events() as poll:\n",
        "    while form_submitted is False:\n",
        "        poll(5)               # Monitor UI events\n",
        "        print('.', end='')   # Show a dot to indicate waiting for input from user\n",
        "        time.sleep(0.3)      # Slight delay to reduce CPU usage\n",
        "\n",
        "    # Return updated agent state with emergency form details\n",
        "  return {\n",
        "        'oncall_cust_info': result\n",
        "    }"
      ],
      "metadata": {
        "id": "G9mU8Wh96_Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accept_user_input_oncall({})"
      ],
      "metadata": {
        "id": "ny_WDU9Z7UNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def escalate_to_oncall_team(support_state: CustomerSupportAgentState) -> CustomerSupportAgentState:\n",
        "    oncall_cust_info = support_state['oncall_cust_info']\n",
        "   # the following response will be shown to the user and can also be sent (customer form inputs) to your on-call doctors\n",
        "    response = (\"Don't worry \" + oncall_cust_info['name'] +\n",
        "               \"!, someone from our on-call expert doctors will be reaching out to your shortly at \" +\n",
        "                oncall_cust_info['number'] +\n",
        "                \" for assistance immediately!\")\n",
        "\n",
        "    # NOTE: You can always add custom code here to call specific APIs like whatsapp to notify your on-call doctors\n",
        "\n",
        "    return {\n",
        "        \"final_response\": response\n",
        "    }"
      ],
      "metadata": {
        "id": "-uhhSeGe8Y4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Create a typed LangGraph state graph using the custom CustomerSupportAgentState\n",
        "customer_support_graph = StateGraph(CustomerSupportAgentState)\n",
        "\n",
        "# Register each functional node in the graph that represents a step in the agent workflow\n",
        "\n",
        "# Step 1: Categorize the incoming query by department (e.g., billing, records, etc.)\n",
        "customer_support_graph.add_node(\"categorize_inquiry\", categorize_inquiry)\n",
        "# Step 2: Analyze the user's sentiment (positive, neutral, negative, distress)\n",
        "customer_support_graph.add_node(\"analyze_inquiry_sentiment\", analyze_inquiry_sentiment)\n",
        "\n",
        "# Step 3a: Accept user input for escalation to human support (for negative sentiment)\n",
        "customer_support_graph.add_node(\"accept_user_input_escalation\", accept_user_input_escalation)\n",
        "# Step 3b: Escalate to a human support agent using the collected details\n",
        "customer_support_graph.add_node(\"escalate_to_human_agent\", escalate_to_human_agent)\n",
        "\n",
        "# Step 4a: Accept user input for escalation to emergency on-call team (for distress sentiment)\n",
        "customer_support_graph.add_node(\"accept_user_input_oncall\", accept_user_input_oncall)\n",
        "# Step 4b: Escalate to on-call emergency doctor team using submitted details\n",
        "customer_support_graph.add_node(\"escalate_to_oncall_team\", escalate_to_oncall_team)\n",
        "\n",
        "# Step 5: Generate a department-specific response using RAG if sentiment is positive or neutral\n",
        "customer_support_graph.add_node(\"generate_department_response\", generate_department_response)\n",
        "\n",
        "# Define the router function that directs the flow based on sentiment and category\n",
        "def determine_route(support_state: CustomerSupportAgentState) -> str:\n",
        "\n",
        "    # Determine the next node based on query sentiment and category.\n",
        "    # - Escalate to human support agent if sentiment is negative i.e fill form for escalation\n",
        "    # - Escalate to emergency on-call team if sentiment is distress i.e fill form for on-call doctors\n",
        "    # - Otherwise, use department-specific RAG response\n",
        "\n",
        "    if support_state[\"query_sentiment\"] == \"Negative\":\n",
        "        return \"accept_user_input_escalation\"\n",
        "    elif support_state[\"query_sentiment\"] == \"Distress\":\n",
        "        return \"accept_user_input_oncall\"\n",
        "    elif support_state[\"query_category\"] in [\"Billing\", \"Appointments\", \"Records\", \"Insurance\"]:\n",
        "        return \"generate_department_response\"\n",
        "\n",
        "# Define the flow of transitions between the nodes in the graph\n",
        "\n",
        "# After categorizing the query, move to sentiment analysis\n",
        "customer_support_graph.add_edge(\"categorize_inquiry\", \"analyze_inquiry_sentiment\")\n",
        "# After sentiment analysis, use conditional routing to determine next steps\n",
        "customer_support_graph.add_conditional_edges(\n",
        "    \"analyze_inquiry_sentiment\",\n",
        "    determine_route,\n",
        "    [\n",
        "        \"accept_user_input_escalation\",\n",
        "        \"accept_user_input_oncall\",\n",
        "        \"generate_department_response\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# If the user input is collected for escalation, route to human agent\n",
        "customer_support_graph.add_edge(\"accept_user_input_escalation\", \"escalate_to_human_agent\")\n",
        "customer_support_graph.add_edge(\"escalate_to_human_agent\", END)\n",
        "\n",
        "# If the user input is collected for on-call emergency, route to on-call team\n",
        "customer_support_graph.add_edge(\"accept_user_input_oncall\", \"escalate_to_oncall_team\")\n",
        "customer_support_graph.add_edge(\"escalate_to_oncall_team\", END)\n",
        "\n",
        "# If sentiment is neutral or positive, generate a department response and finish\n",
        "customer_support_graph.add_edge(\"generate_department_response\", END)\n",
        "\n",
        "# Set the starting point of the workflow\n",
        "customer_support_graph.set_entry_point(\"categorize_inquiry\")\n",
        "\n",
        "# Compile the graph\n",
        "memory = MemorySaver()\n",
        "form_submitted = False # initially no form has been submitted\n",
        "compiled_support_agent = customer_support_graph.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "_AJBc2GK-rLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "print(inspect.getsource(categorize_inquiry))"
      ],
      "metadata": {
        "id": "7i5BL9oLk937"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image, Markdown\n",
        "\n",
        "display(Image(compiled_support_agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "IvfjRo7l-0ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_support_agent(agent, prompt, user_session_id, verbose=False):\n",
        "    events = agent.stream(\n",
        "        {\"customer_query\": prompt}, # initial state of the agent\n",
        "        {\"configurable\": {\"thread_id\": user_session_id}},\n",
        "        stream_mode=\"values\",\n",
        "    )\n",
        "\n",
        "    print('Running Agent. Please wait...')\n",
        "    for event in events:\n",
        "        if verbose:\n",
        "                print(event)\n",
        "\n",
        "\n",
        "    print('\\nFinal Response:')\n",
        "    display(Markdown(event['final_response']))"
      ],
      "metadata": {
        "id": "5i3Sc-bM_LJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[item['text'] for item in knowledge_base]"
      ],
      "metadata": {
        "id": "DHoRJimT_OIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_agent_wrapper(query: str, user_session_id: str, verbose: bool = False):\n",
        "    \"\"\"\n",
        "    Wrapper that bypasses interactive forms during testing\n",
        "    \"\"\"\n",
        "    # Inject test mode into state\n",
        "    result = compiled_support_agent.invoke(\n",
        "        {\n",
        "            \"customer_query\": query,\n",
        "            \"test_mode\": True  # ‚Üê Tell agent we're testing\n",
        "        },\n",
        "        config={\"configurable\": {\"thread_id\": user_session_id}}\n",
        "    )\n",
        "    return result"
      ],
      "metadata": {
        "id": "ocnA9CM1Rc3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from automated_agent_testing_framework import *\n",
        "\n",
        "def test_agent_wrapper(query: str, user_session_id: str, verbose: bool = False):\n",
        "    result = compiled_support_agent.invoke(\n",
        "        {\n",
        "            \"customer_query\": query,\n",
        "            \"test_mode\": True  # ‚Üê Enables test mode\n",
        "        },\n",
        "        config={\"configurable\": {\"thread_id\": user_session_id}}\n",
        "    )\n",
        "    return result\n",
        "\n",
        "# Run tests\n",
        "test_cases = get_comprehensive_test_cases()\n",
        "runner = AgentTestRunner(test_agent_wrapper)\n",
        "\n",
        "results = runner.run_test_suite(\n",
        "    test_cases,\n",
        "    verbose=True,\n",
        "    use_llm_judge=False  # Start with heuristics\n",
        ")\n",
        "\n",
        "print(\"\\n\" + TestReporter.generate_summary_report(results))"
      ],
      "metadata": {
        "id": "Fzk2oORNkn5H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}